{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c4ee686",
   "metadata": {},
   "source": [
    "## Ch5 lab: cross-validation and the bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c102d2",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7082aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy # importing SciPu library to do scientific computing, e.g. numerical integration, optimisaiton, linear algebra\n",
    "import pandas as pd \n",
    "import math\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import OrderedDict # provides specialised container datatypes that are alternatives to built-in data structures like lists, tuples, dictionaries etc\n",
    "\n",
    "# OrderedDict is a dictionary subclass that maintains the order of inserted items, useful for when the order of elements is important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d357d87c",
   "metadata": {},
   "source": [
    "### 2. The validation set approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89eb42e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 9)\n",
      "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
      "0  18.0          8         307.0       130.0    3504          12.0    70   \n",
      "1  15.0          8         350.0       165.0    3693          11.5    70   \n",
      "2  18.0          8         318.0       150.0    3436          11.0    70   \n",
      "3  16.0          8         304.0       150.0    3433          12.0    70   \n",
      "4  17.0          8         302.0       140.0    3449          10.5    70   \n",
      "\n",
      "   origin                       name  \n",
      "0       1  chevrolet chevelle malibu  \n",
      "1       1          buick skylark 320  \n",
      "2       1         plymouth satellite  \n",
      "3       1              amc rebel sst  \n",
      "4       1                ford torino  \n"
     ]
    }
   ],
   "source": [
    "auto = pd.read_csv('data/Auto.csv', header = 0, na_values = '?')\n",
    "auto = auto.dropna().reset_index(drop = True) # dropping the observations with NA values and reindex the observations from 0\n",
    "print(auto.shape)\n",
    "print(auto.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fefc019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a training set\n",
    "np.random.seed(1) # setting seed to ensure reproducibility in number generation\n",
    "train = np.random.choice(auto.shape[0], 196, replace = False) # creating an array 'train' containing a random, non-repeated subset\n",
    "select = np.in1d(range(auto.shape[0]), train) # creates a boolean arrray 'select' to show whether indicies in auto are in the train array or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72adf1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.620\n",
      "Model:                            OLS   Adj. R-squared:                  0.618\n",
      "Method:                 Least Squares   F-statistic:                     316.4\n",
      "Date:                Mon, 15 May 2023   Prob (F-statistic):           1.28e-42\n",
      "Time:                        09:48:01   Log-Likelihood:                -592.07\n",
      "No. Observations:                 196   AIC:                             1188.\n",
      "Df Residuals:                     194   BIC:                             1195.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     40.3338      1.023     39.416      0.000      38.316      42.352\n",
      "horsepower    -0.1596      0.009    -17.788      0.000      -0.177      -0.142\n",
      "==============================================================================\n",
      "Omnibus:                        8.393   Durbin-Watson:                   1.061\n",
      "Prob(Omnibus):                  0.015   Jarque-Bera (JB):                8.787\n",
      "Skew:                           0.516   Prob(JB):                       0.0124\n",
      "Kurtosis:                       2.899   Cond. No.                         328.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#Â building the model\n",
    "lm = smf.ols ('mpg~horsepower', data = auto[select]).fit() # selecting a subset of the auto data 'select' so that we only receive our training data\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a08e9de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Test error for 1st order model --------\n",
      "23.361902892587235\n"
     ]
    }
   ],
   "source": [
    "# getting predictions for all observations in the dataset, using '~' to exclude results from the training samples\n",
    "preds = lm.predict(auto)\n",
    "square_error = (auto['mpg'] - preds)**2 # calculates squared error between actual targer values and predicted values\n",
    "print('-------- Test error for 1st order model --------')\n",
    "print(np.mean(square_error[~select])) # calculates the mean of the squared error for rows that... \n",
    "# do not satisfy the condition represented by '~select', i.e. rows that weren't in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "721c664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Test error for 2nd order--------\n",
      "20.25269085835007\n"
     ]
    }
   ],
   "source": [
    "# building a model with 2nd order of features\n",
    "lm2 = smf.ols ('mpg~horsepower + I(horsepower ** 2.0)', data = auto[select]).fit()\n",
    "\n",
    "# getting predictions for all observations in the dataset, again using '~' to exclude training samples\n",
    "preds = lm2.predict(auto)\n",
    "square_error = (auto['mpg'] - preds)**2\n",
    "print('--------Test error for 2nd order--------')\n",
    "print(square_error[~select].mean()) # equivalent to above version, just different syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "120abb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Test error for 2nd order--------\n",
      "20.325609365773644\n"
     ]
    }
   ],
   "source": [
    "# building a model with 3rd order of features\n",
    "lm3 = smf.ols('mpg~horsepower + I(horsepower ** 2.0) + I(horsepower ** 3.0)', data = auto[select]).fit()\n",
    "\n",
    "# getting predictions for all observations, excluding training samples\n",
    "preds = lm3.predict(auto)\n",
    "square_error = (auto['mpg'] - preds) ** 2 \n",
    "print('--------Test error for 2nd order--------')\n",
    "print(square_error[~select].mean()) # equivalent to above version, just different syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed774f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.722\n",
      "Model:                            OLS   Adj. R-squared:                  0.717\n",
      "Method:                 Least Squares   F-statistic:                     165.9\n",
      "Date:                Mon, 15 May 2023   Prob (F-statistic):           4.60e-53\n",
      "Time:                        09:50:12   Log-Likelihood:                -561.56\n",
      "No. Observations:                 196   AIC:                             1131.\n",
      "Df Residuals:                     192   BIC:                             1144.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept               66.5200      6.310     10.541      0.000      54.073      78.967\n",
      "horsepower              -0.6868      0.162     -4.238      0.000      -1.006      -0.367\n",
      "I(horsepower ** 2.0)     0.0028      0.001      2.157      0.032       0.000       0.005\n",
      "I(horsepower ** 3.0) -3.524e-06   3.27e-06     -1.078      0.282   -9.97e-06    2.92e-06\n",
      "==============================================================================\n",
      "Omnibus:                        9.054   Durbin-Watson:                   1.328\n",
      "Prob(Omnibus):                  0.011   Jarque-Bera (JB):               15.936\n",
      "Skew:                           0.174   Prob(JB):                     0.000346\n",
      "Kurtosis:                       4.353   Cond. No.                     5.83e+07\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.83e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "print(lm3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec3c2d",
   "metadata": {},
   "source": [
    "### 3. Leave-one-out cross validation\n",
    "\n",
    "Leave-one-out cross-validation (LOOCV) is a technique used to estimate the performance of a machine learning model. In LOOCV, the dataset is divided into multiple subsets, each consisting of <b>all the samples except one</b>. For each subset, a model is trained using the remaining samples and then evaluated on the sample that was left out. This process is repeated for each sample in the dataset, and the evaluation results are averaged to obtain an overall performance estimate.\n",
    "\n",
    "In more detail, the LOOCV process is as follows:\n",
    "- Starting with the original dataset, LOOCV iterates over each sample one by one, designating it as the validation or test sample.\n",
    "\n",
    "- The remaining samples in the dataset are used as the training data to build a model. This ensures that the training model has access to as much data as possible, similar to training on the entire dataset.\n",
    "\n",
    "- The model is then used to predict the outcome or target variable for the validation sample.\n",
    "\n",
    "- The predicted value is compared to the actual value of the validation sample, and the performance metric of interest (e.g., accuracy, mean squared error, etc.) is computed.\n",
    "\n",
    "- Steps 2-4 are repeated for each sample in the dataset, with each sample taking a turn as the validation sample.\n",
    "\n",
    "- The performance metrics obtained for each validation sample are then averaged to obtain a single performance estimate for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fb17f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept     39.935861\n",
      "horsepower    -0.157845\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# OLS fit \n",
    "ols_fit = smf.ols ('mpg~horsepower', data = auto).fit()\n",
    "print(ols_fit.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5a632ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept     39.935861\n",
      "horsepower    -0.157845\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# GLM fit. Compare with OLS fit, the coeffs are the same\n",
    "glm_fit = smf.glm('mpg~horsepower', data = auto).fit()\n",
    "print(glm_fit.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "314cd5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using libraries / modules already imported to perform kfold validation in Python, as a reminder they are:\n",
    "# from sklearn.model_selection import KFold, cross_val_score\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0062fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.93586102117047\n",
      "[-0.15784473]\n"
     ]
    }
   ],
   "source": [
    "# retraining the model in sklearn\n",
    "x = pd.DataFrame(auto.horsepower)\n",
    "y = auto.mpg\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f00d0f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.231513517929226\n"
     ]
    }
   ],
   "source": [
    "# using kfold validation to evaluate the performance of the model\n",
    "k_fold = KFold(n_splits=x.shape[0]) # initialised a KFold object 'k_fold'. 'n_splits' set to no. samples in input data 'x.shape[0]'\n",
    "test = cross_val_score(model, x, y, cv=k_fold,  scoring = 'neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# applies cross-validation with following parameters:\n",
    "# 'model' = object / estimator to be evaluated, as defined in code block above\n",
    "# 'x' = input features / predictor variables\n",
    "# 'y' = target variables / labels\n",
    "# 'cv' = the cross-validation strategy or object, in this case 'f_fold' as defined on previous line\n",
    "# 'scoring' = scoring metric used to evaluate model's performance, in this case negative mean squared error\n",
    "# 'n_jobs' = no. CPU cores to use for computation, value of -1 indicates using all available CPU cores\n",
    "\n",
    "print(np.mean(-test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cdff3d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('1', 24.231513517929226), ('3', 19.334984064118647), ('5', 19.033206870017146), ('7', 19.125446846687602), ('9', 19.13392619499544), ('11', 19.136033100530025), ('13', 27.76341651771519), ('15', 35.29333367792178), ('17', 43.65451056724629), ('19', 60.96829353029924)])\n"
     ]
    }
   ],
   "source": [
    "# using pipline tool for a higher order polynomial fit\n",
    "\n",
    "A = OrderedDict()\n",
    "n_split = x.shape[0] # assigns no. samples in input data 'x' to the variable 'n_split', represents no. splits / folds to be used in cross-validation\n",
    "for porder in range(1, 21, 2): # range values from 1 to 21 with step of 2\n",
    "    model = Pipeline([('poly', PolynomialFeatures(degree = porder)), ('linear', LinearRegression())])\n",
    "    k_fold = KFold(n_splits = n_split)\n",
    "    test = cross_val_score(model, x, y, cv = k_fold, scoring = 'neg_mean_squared_error', n_jobs=-1)\n",
    "    A[str(porder)] = np.mean(-test)\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06610dd",
   "metadata": {},
   "source": [
    "#### Notes on interpretation\n",
    "\n",
    "Overall, this code block performs polynomial regression with different polynomial orders, applies k-fold cross-validation to evaluate the performance of each model using negative mean squared error, and stores the mean squared error values in an OrderedDict. Finally, it prints the OrderedDict containing the mean squared error values for different polynomial orders.\n",
    "\n",
    "- <code>model = Pipeline([('poly', PolynomialFeatures(degree=porder)), ('linear', LinearRegression())])</code>: creates a pipeline for the model using the 'Pipeline' class from scikit-learn. The pipeline consists of two steps: \n",
    "    - Polynomial feature transformation using PolynomialFeatures with the specified degree porder\n",
    "    - Linear regression using LinearRegression.\n",
    "\n",
    "- <code>k_fold = KFold(n_splits=n_split) </code>: initializes a KFold object 'k_fold' for performing k-fold cross-validation. The 'n_splits' parameter is set to the number of samples in the input data ('n_split'), indicating that each sample will be used as the test set once in each fold.\n",
    "\n",
    "- <code>test = cross_val_score(model, x, y, cv=k_fold, scoring='neg_mean_squared_error', n_jobs=-1)</code>: applies cross-validation to evaluate the performance of the pipeline model. The 'cross_val_score' function performs cross-validation by splitting the data into training and test sets according to the specified cross-validation strategy ('k_fold'), trains the pipeline model on each training set, evaluates the model's performance on the corresponding test set using negative mean squared error, and returns the scores for each fold. The negative mean squared error values for each fold are stored in the test variable.\n",
    "\n",
    "- <code>A[str(porder)] = np.mean(-test)</code>: calculates the mean of the negative mean squared error values obtained from cross-validation ('-test') and stores it in the 'OrderedDict' 'A'. The polynomial order porder is used as the key in A to associate it with the corresponding mean squared error value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acf3fbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 24.231513517929226\n",
      "3: 19.334984064118647\n",
      "5: 19.033206870017146\n",
      "7: 19.125446846687602\n",
      "9: 19.13392619499544\n",
      "11: 19.136033100530025\n",
      "13: 27.76341651771519\n",
      "15: 35.29333367792178\n",
      "17: 43.65451056724629\n",
      "19: 60.96829353029924\n"
     ]
    }
   ],
   "source": [
    "# printing results from OrderedDict in a more readable format, one below another:\n",
    "\n",
    "for key, value in A.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab56f0",
   "metadata": {},
   "source": [
    "### 4. Boostrap sampling\n",
    "\n",
    "Boostrap sampling involves creating multiple resamples of the dataset by randomly selecting observations from the original dataset with replacement. Each resample is of the same size as the original dataset, but some observations may be selected multiple times, while others may not be selected at all. This process allows for the creation of multiple datasets that are similar to the original dataset, but with slight variatinos due to the sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "056fbb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio = pd.read_csv('data/Portfolio.csv', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49b3e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function 'alpha_fn' which calculates value called 'alpha' using a given dataset and an index of selected samples\n",
    "def alpha_fn(data, index):\n",
    "    X = data.X.iloc[index]\n",
    "    Y = data.Y.iloc[index]\n",
    "    return (np.var(Y) - np.cov(X,Y)[0,1])/(np.var(X) + np.var(Y) - 2 * np.cov(X, Y)[0,1])\n",
    "\n",
    "# last line calculates the value of 'alpha', computing the numerator and denominator then dividing them to obtain final value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84f4d112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5766511516104116"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling the function 'alpha_fn' with two arguments, 'portfolio' and 'range'\n",
    "alpha_fn(portfolio, range(0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21867b4a",
   "metadata": {},
   "source": [
    "#### Notes on interpretation for block above\n",
    "\n",
    "By calling <code>alpha_fn(portfolio, range(0,100))</code>, the <code>alpha_fn</code> function is invoked with the portfolio dataset and a sequence of indices from 0 to 99. This means that the function will calculate the alpha value using the samples at indices 0 to 99 from the X and Y variables in the portfolio dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74e0ae17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  1,  1,  1,  4,  5,  5,  7,  7,  7, 11, 11, 13, 14, 16, 16,\n",
       "       17, 18, 21, 21, 21, 22, 24, 24, 25, 25, 26, 27, 27, 28, 31, 31, 32,\n",
       "       32, 32, 32, 33, 33, 34, 34, 35, 36, 37, 38, 38, 38, 40, 40, 41, 42,\n",
       "       43, 44, 45, 45, 45, 46, 48, 48, 50, 51, 52, 57, 58, 61, 62, 62, 64,\n",
       "       66, 67, 67, 68, 68, 68, 71, 71, 72, 74, 75, 76, 76, 82, 82, 84, 85,\n",
       "       88, 88, 88, 89, 92, 92, 94, 96, 96, 96, 97, 97, 97, 97, 99])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generates a random sample of 100 numbers chosen with replacement from 0 to 99\n",
    "# then stores the numbers in ascending order using 'np.sort'\n",
    "np.sort(np.random.choice(range(0,100), size = 100, replace = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bccfffd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6301886002042987"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling the 'alpha_fn' again with two arguments, this time using the randomly generated range above\n",
    "# results in alpha value calculates using random subset of samples from X and Y variables in the portfolio dataset\n",
    "alpha_fn(portfolio, np.random.choice(range(0,100), size = 100, replace = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f47aed1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mean': 0.5815356030576532, 'STD': 0.09187477005226481}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining function to perform bootstrapping on a given dataset using a specified input function\n",
    "# the function conducts bootstrapping for a specified no. iterations and returns the mean and std of the resulting statistic\n",
    "\n",
    "def boot_python(data, input_fun, iteration):\n",
    "    n = portfolio.shape[0]\n",
    "    idx = np.random.randint(0, n, (iteration, n))\n",
    "    stat = np.zeros(iteration)\n",
    "    for i in range(len(idx)):\n",
    "        stat[i] = input_fun(data, idx[i]) \n",
    "    return {'Mean': np.mean(stat), 'STD': np.std(stat)}\n",
    "    \n",
    "boot_python(portfolio, alpha_fn, 1000) # applying the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7063f",
   "metadata": {},
   "source": [
    "#### Notes on interpretation\n",
    "\n",
    "- <code>def boot_python(data, input_fun, iteration):</code>: This line defines the function boot_python that takes three arguments: data, which represents the dataset, input_fun, which is the input function used to calculate a statistic, and iteration, the number of bootstrap iterations.\n",
    "\n",
    "- <code>n = Portfolio.shape[0]</code>: This line assumes there is a variable or object named Portfolio and assigns the number of rows in the Portfolio dataset to the variable n.\n",
    "\n",
    "- <code>idx = np.random.randint(0, n, (iteration, n))</code>: This line generates a two-dimensional array idx using np.random.randint. Each row in idx contains a bootstrap sample of indices randomly selected from the range 0 to n-1 (the number of rows in the dataset). The iteration parameter determines the number of rows, and n specifies the range of random integers.\n",
    "\n",
    "- <code>stat = np.zeros(iteration)</code>: This line initializes an array stat of size iteration filled with zeros. This array will store the statistics calculated by the input function for each bootstrap sample.\n",
    "\n",
    "- <code>for i in range(len(idx)):</code>: This line starts a loop over the length of idx, iterating from 0 to len(idx)-1.\n",
    "\n",
    "- <code>stat[i] = input_fun(data, idx[i])</code>: Within the loop, this line calls the input function input_fun with the data dataset and the i-th row of idx. It calculates a statistic using the specified input function and assigns the result to the i-th element of the stat array.\n",
    "\n",
    "- <code>return {'Mean': np.mean(stat), 'STD': np.std(stat)}</code>: This line returns a dictionary containing the mean and standard deviation of the stat array. The mean and standard deviation are calculated using np.mean and np.std, respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
